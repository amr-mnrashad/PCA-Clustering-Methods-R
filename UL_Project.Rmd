---
title: An R Markdown document converted from "UL_Project.ipynb"
output: html_document
---

<h2> <span style="color: Red;"> UniMi | SL | Supervised Learning | Amr Rashad FIFA 23 Player Attributes dataset </span> </h2>

## <span style="color: LightBlue;"> 0. Outline </span>

- The Curse of Dimensionality is a serious problem, and our dataset suffers from high-dimensionality. To avoid such problem, I will be using Principal Component Analysis before clustering, in order to reduce number of dimensions, but also to reduce unnecessary noise of data. This will possibly improve the clustering performance for the following methods that are going to be used: 
    - K-means clustering
    - Hierarchical clustering

## <span style="color: IndianRed;"> 1. Loading & Preparing The Dataset: </span>

### <span style="color: LightSalmon;"> *1.1 Load Required Libraries:* </span>

```{r}
library(MASS)
library(ggpubr)
library(olsrr)
library(dplyr)
library(purrr)
library (DescTools)
library(gplots)
library(car)
library(Hmisc)
library(PerformanceAnalytics)
library(olsrr)
library(ISLR)
library(rpart)
library(rpart.plot)
library(party)
library("partykit")
library(caTools)
library(caret)
library(randomForest)
library(ranger)
library(corrplot)
library(tidyverse)
library("magrittr")
library(gridExtra)
library(cowplot)
library(pROC)
library(ggcorrplot)
library(tree)
library(leaps)
library(class)
library("pROC")
library(lares)
library(rgl)
library(cluster)
library(plotly)
library(dendextend)
```

### <span style="color: LightSalmon;"> *1.2 Load Dataset:* </span>

```{r}
FPA <- read.csv(file='FIFA23_Player_Attributes.csv', stringsAsFactors=TRUE)
```

```{r}
str(FPA)
```

```{r}
head(FPA)
```

```{r}
FPA["Player"] <- NA
FPA <- FPA %>% relocate(Player, .before = Ã¯..Full.Name)
```

```{r}
for(i in 1:nrow(FPA)){
    FPA$Player[i] = i 
}
FPA <- FPA[-2]
head(FPA)
```

### <span style="color: LightSalmon;"> *1.3 Data Preparation:* </span>

```{r}
FPA <- data.frame(FPA)
```

```{r}
#checking if there exist na values
apply(FPA, 2, function(x) any(is.na(x)))
```

```{r}
head(FPA)
```

```{r}
#Seeing the structure of the dataset
df_str(FPA, return = "plot")
```

## <span style="color: IndianRed;"> 2. Principal Component Analysis: </span>

```{r}
row.names(FPA)<-FPA$Player
FPA<-FPA[,-1]
head(FPA)
```

```{r}
pca <- prcomp(FPA[-1], scale = TRUE)
plot(pca$x[,1], pca$x[,2])
```

```{r}
plot(pca, type = "lines", main = "Scree Diagram")
```

```{r}
summary(pca)
```

It seems like the best number of principal components we can choose that would explain the maximal variation in the data with minimal loss of information is 3.

```{r}
#Identifying the X, Y, Z coordinates of each data point (in our case it's the players), where each row represents a player
pca.data <- data.frame(rownames(pca$x), X = pca$x[,1], Y = pca$x[,2], Z = pca$x[,3])
head(pca.data)
```

```{r}
pca.var <- pca$sdev^2
pca.var.per <-round(pca.var/sum(pca.var)*100, 1)
ggplot(data = pca.data, aes(x = X, y = Y, label = rownames(pca$x)))+
geom_text()+
xlab(paste("PC1 - ", pca.var.per[1], "%", sep = ""))+
ylab(paste("PC2 - ", pca.var.per[2], "%", sep = ""))+
theme_bw()+
ggtitle("PCA Graph")
```

- The x-axis tells us what percentage of the variation in the original data that PC1 accounts for
- The y-axis tells us what percentage of the variation in the original data that PC2 accounts for

- Let's look at how to use the loading scores to determine which attributes have the largest effect on where players are plotted in the PCA plot
- Let's first look at the loading scores for PC1 since it accounts for 49% of the variation in the data

```{r}
loading.scores <- pca$rotation[,1]
attribute.scores <- abs(loading.scores)
#sorting the magnitude of the loading scores from high to low
attribute.scores.ranked <- sort(attribute.scores, decreasing = TRUE)
#now we get the names of the top 10 attributes with the largest loading score magnitudes
top10.attributes <- names(attribute.scores.ranked[1:10])
top10.attributes
```

```{r}
#We try to see which of the top 10 attributes have positive(resulting in pushing the datapoints to the right)/negative (resulting in pushing the datapoints to the left) loading scores
pca$rotation[top10.attributes, 1]
```

- Let's now look at the loading scores for PC2 which accounts for 17.2% of the variation in the data

```{r}
loading.scores <- pca$rotation[,2]
attribute.scores <- abs(loading.scores)
#sorting the magnitude of the loading scores from high to low
attribute.scores.ranked <- sort(attribute.scores, decreasing = TRUE)
#now we get the names of the top 10 attributes with the largest loading score magnitudes
top10.attributes <- names(attribute.scores.ranked[1:10])
top10.attributes
```

```{r}
#We try to see which of the top 10 attributes have positive(resulting in pushing the datapoints to the right)/negative (resulting in pushing the datapoints to the left) loading scores
pca$rotation[top10.attributes, 2]
```

- Finally, let's look at the loading scores for PC3 which accounts for 10.1% of the variation in the data

```{r}
loading.scores <- pca$rotation[,3]
attribute.scores <- abs(loading.scores)
#sorting the magnitude of the loading scores from high to low
attribute.scores.ranked <- sort(attribute.scores, decreasing = TRUE)
#now we get the names of the top 10 attributes with the largest loading score magnitudes
top10.attributes <- names(attribute.scores.ranked[1:10])
top10.attributes
```

```{r}
#We try to see which of the top 10 attributes have positive(resulting in pushing the datapoints to the right)/negative (resulting in pushing the datapoints to the left) loading scores
pca$rotation[top10.attributes, 3]
```

Let's now draw a biplot to determine how exactly the players are distributed based on the loading scores of PC1 and PC2

```{r}
#options(repr.plot.width = 15, repr.plot.height =15)
biplot(pca)
```

- From the biplot graph we can make few interesting observations:
    - on the right hand corner we can see that the goalkeepers are clustered together
    - The other main cluster we can observe as well is on the left, which represents the field players
- Looking at the loading vectors we can also notice the following:
    - on the top left corner we can see the following attributes: sliding tackle, defending total, standing tackle, interceptions, marking, aggression, strength... All of these attributes mainly describe defenders, hence we can infer that the top part of the left cluster is mostly composed of defenders
    - As we go down the left clusters, we can also determine that the players become classified as more offensive, based on the following attributes we can see on the bottom left corner: finishing, shooting total, passing total, pace total, vision, long shots, dribbling...

An interesting way to confirm what we've just observed is to plot the PCA according to the players positions

```{r}
FPA$pc1 <- pca$x[,1]
FPA$pc2 <- pca$x[,2]
FPA$pc3 <- pca$x[,3]
ggplot(data = FPA, aes(x = pc1, y = pc2, color = Best.Position, label = rownames(pca$x)))+
geom_text()+
xlab(paste("PC1 - ", pca.var.per[1], "%", sep = ""))+
ylab(paste("PC2 - ", pca.var.per[2], "%", sep = ""))+
theme_bw()+
ggtitle("PCA Graph")
```

- Again, we can clearly see that the goalkeepers are grouped in one cluster while in the other cluster, the top part is mainly formed with defenders and the bottom part is mainly attackers.
- What can also be interesting to note is that we have wingbacks all over the cluster, which makes sense because in football usually there would be wingbacks who are more offensive and other wingbacks are more defensive. How a player is categorized as offensive or defensive is based on how they play in real life, so the style of play could be influenced by several factors, such as formation, league... It would be interesting to retrieve a dataset where we can identify which leagues for example have the most players who prefer to play an offensive style of play, but that's an analysis to be conducted some other time.

A final interesting 3D visualization of our PCA plot, showing PC1, PC2 and PC3 to get a better idea of the variation in the dataset.

```{r}
fig <- plot_ly(FPA, x = ~pc1, y = ~pc2, z = ~pc3, color = ~Best.Position)
fig <- fig %>% add_markers()
fig <- fig %>% layout(scene = list(xaxis = list(title = 'PC1'),
                     yaxis = list(title = 'PC2'),
                     zaxis = list(title = 'PC3')))
fig
```

## <span style="color: IndianRed;"> 3. Clustering: </span>

### <span style="color: LightSalmon;"> *3.1 K-means Clustering:* </span>

- Based on our general observation from running the PCA algorithm, we observe that the players are seperated into two groups: players(attackers, midfielders and defenders) and goalkeepers, and the players are seperated into two main parts, offensive and defensive players. Let's try and apply the k-means clustering at first using this observation to our advantage.
- However as we've decided, the optimal number of dimensions we selected after running the PCA algorithm is 3, we will come to that later...

```{r}
set.seed(123)
#calculating distances between players where the number of clusters is 3, using the principal component scores we have where the number of dimensions is 2.
kc <- kmeans(pca$x[,1:2], centers = 3)
#number of datapoints in each cluster
table(kc$cluster)
```

- As we can see right away, the distribution of data points between the clusters is not even, so that might be an indicator telling us that we might need to change the number of our clusters. However let's see where will this lead us.

```{r}
#Plotting the clusters with their centers
plot(pca$x[,1:2],col=factor(kc$cluster)) + points(kc$centers, pch = 8, cex = 3, col = "blue")
```

It seems that we were on the right path with our analysis, where we can see that the big cluster is divided into three subparts: offensive(black), defensive(red) and goalkeepers(green) 

What we just did is select the number of clusters (k) based on our intuition. However the question is, how can we make this more accurate? i.e how to choose the best number of clusters?
- one way we can do that is to minimize the total within-cluster variation.
    - Within-cluster variation for a single cluster can simply be defined as the sum of squares from the cluster mean, which in this case is the centroid we defined in the k-means algorithm. 
    - The total within-cluster variation is then the sum of within-cluster variations for each cluster.
- Using an Elbow plot we can determine which is the best number of clusters that we think we will be satisfied with

```{r}
k.max <- 15
data <- pca$x[,1:2]
wss <- sapply(1:k.max, 
              function(k){
               kmeans(data, k, nstart=50,iter.max = 100 )$tot.withinss
               #if (kc$ifault==4) { stop("Failed in Quick-Transfer"); }
})
df.wss <- data.frame(wss)
df.wss
```

```{r}
plot(1:k.max, wss/1e+05,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```

- Looking at both the elbow plot & the total within-cluster variation scores, It seems like the Total_ss tends to change slowly starting from k = 6

```{r}
set.seed(123)
kc <- kmeans(pca$x[,1:2], centers = 6)
#number of datapoints in each cluster
table(kc$cluster)
```

It seems like now we have a some-what average number of datapoints in each cluster.

```{r}
#Plotting the clusters with their centers
plot(pca$x[,1:2],col=factor(kc$cluster)) + points(kc$centers, pch = 8, cex = 3, col = "blue")
```

- Seeing how the clusters are formed, we can clearly see the goalkeepers are identified as a one cluster and for the others, they represent how players are less defensive and more offensive gradually. For instance, the very top cluster seems to be representing the defenders, and as we go down, the defensive attributes of the players decrease gradually while the offensive attributes increase gradually as well. 

- Now let's apply the same thought process we just did, but using the optimal number of dimensions (3).

```{r}
set.seed(123)
#calculating distances between players where the number of clusters is 3, using the principal component scores we have where the optimal number of dimensions is 3.
kc.3d <- kmeans(pca$x[,1:3], centers = 3)
#number of datapoints in each cluster
table(kc.3d$cluster)
```

Again, it seems like there's an imbalance between the distribution of the datapoints in the clusters, but as we did before, let's see where this will lead us.

```{r}
df <- data.frame(pca$x)
head(df$PC1)
```

```{r}
fig <- plot_ly(df, x = ~PC1, y = ~PC2, z = ~PC3, color =kc.3d$cluster)
fig <- fig %>% add_markers()
fig <- fig %>% layout(scene = list(xaxis = list(title = 'PC1'),
                     yaxis = list(title = 'PC2'),
                     zaxis = list(title = 'PC3')))
fig
```

- Same observation we achieved when using 2 dimensions.

- let's find the optimal number of clusters using the total within-cluster varation and the elbow plot

```{r}
k.max <- 15
data <- pca$x[,1:3]
wss <- sapply(1:k.max, 
              function(k){
               kmeans(data, k, nstart=50,iter.max = 100 )$tot.withinss
               #if (kc$ifault==4) { stop("Failed in Quick-Transfer"); }
})
df.wss <- data.frame(wss)
df.wss
```

```{r}
plot(1:k.max, wss/1e+05,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```

- Looking at both the elbow plot & the total within-cluster variation scores, and again it seems like the Total_ss tends to change slowly starting from k = 6

```{r}
set.seed(123)
#calculating distances between players where the optimal number of clusters is 4, using the principal component scores we have where the optimal number of dimensions is 3.
kc.3d <- kmeans(pca$x[,1:3], centers = 6)
#number of datapoints in each cluster
table(kc.3d$cluster)
```

- We can observe that we have again a some-what average number of datapoints in each cluster.

```{r}
fig <- plot_ly(df, x = ~PC1, y = ~PC2, z = ~PC3, color =kc.3d$cluster)
fig <- fig %>% add_markers()
fig <- fig %>% layout(scene = list(xaxis = list(title = 'PC1'),
                     yaxis = list(title = 'PC2'),
                     zaxis = list(title = 'PC3')))
fig
```

- Let's use another clustering method but without pre-specifying the number of cluster and later we can compare results.

### <span style="color: LightSalmon;"> *3.2 Hierarchical Clustering:* </span>

- We're going to use the PCA scores to our advantage, where we have been able to reduce the dimensionality of our dataset to just 3 dimensions.

```{r}
scores <- data.frame(pca$x)
```

```{r}
head(scores[,1:2])
```

- First let's apply the hierarchical clustering using only two dimensions to have a general idea on how the dataset will be clustered

```{r}
dm <- dist(scores[,1:2])
```

```{r}
hc<-hclust(dm, method="complete")
```

```{r}
## agglomeration program function
agglo <- function(hc){
  data.frame(row.names=paste0("Cluster",seq_along(hc$height)),
             height=hc$height,
             components=ifelse(hc$merge<0, 
                               hc$labels[abs(hc$merge)], paste0("Cluster",hc$merge)),
             stringsAsFactors=FALSE) }
head(agglo(hc))
```

```{r}
plot(hc)
```

- Seems like we're already off to a good start, as we can observe from the dendrogram that there are 6 main clusters (same as what we found out using k-means clustering).

```{r}
hc<-color_branches(hc, k=6)
```

```{r}
plot(hc)
```

```{r}
clu <- cutree(hc, k=6)
plot(scores[,1:2], col=clu, main="Complete linkage")
```

- Same observation we had when using k-means clustering. Now let's shift to using 3 dimensions and see how are our data points (players) clustered.

```{r}
dm.3d <- dist(scores[,1:3])
```

```{r}
hc.3d<-hclust(dm.3d, method="complete")
```

```{r}
## agglomeration program function
agglo <- function(hc){
  data.frame(row.names=paste0("Cluster",seq_along(hc$height)),
             height=hc$height,
             components=ifelse(hc$merge<0, 
                               hc$labels[abs(hc$merge)], paste0("Cluster",hc$merge)),
             stringsAsFactors=FALSE) }
head(agglo(hc.3d))
```

```{r}
plot(hc.3d)
```

- Again it looks like the main clusters in our dendrogram are 6. 

```{r}
hc.3d<-color_branches(hc, k=6)
```

```{r}
plot(hc.3d)
```

```{r}
clu <- cutree(hc.3d, k=6)
```

```{r}
fig <- plot_ly(scores, x = ~PC1, y = ~PC2, z = ~PC3, color =clu)
fig <- fig %>% add_markers()
fig <- fig %>% layout(scene = list(xaxis = list(title = 'PC1'),
                     yaxis = list(title = 'PC2'),
                     zaxis = list(title = 'PC3')))
fig
```

- We can distinguish that using k-means clustering and Hierchical clustering we end up with somewhat the same conclusion, which is, our datapoints are best divided into 6 main clusters. Where the first one is the goalkeepers and the other 5 clusters represent the rest of the players. Each cluster represent a set of players with certain attributes as their strong features. For example some players have higher scores in their defensive attributes than others, which means they're more suited in playing a more defensive playstyle, example defenders or defensive midfielders. While others are more suited in playing a more offensive playstyle... However looking at the biplot we drew earlier in the PCA section, we can actually observe that being offensive or defensive player is a very general/broad observation. There are other measurements that describe the player's style of play, for example how creative is s/he, how versatile is s/he, how clinical is s/he. All of these measurements/metrics explain to us the meaning behind how the players are being clustered using both of the techniques we talked about earlier.

